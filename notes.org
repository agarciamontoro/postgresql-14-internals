:PROPERTIES:
:ID:       6dc6dbab-083c-41ef-bf78-74a6e5ec50e4
:END:
#+title: PostgreSQL 14 Internals
#+filetags: book
* Metadata

- Author: [[id:a4d3b9c4-8146-42c4-af83-3ac12fd164cd][Egor Rogov]]
- Began reading: <2022-07-27 miÃ©>
- Ended reading:
- Rating: /3

* Notes

** Chapter 1: Introduction

*** Data Organization

The logical distribution of objects are determined by:
- Databases
- Schemas

The physical layout is determined by:
- Tablespaces

**** Databases
All data from a database cluster (a collection of databases served by a single PostgreSQL server) is contained in a directory usually called =PGDATA=. Initially, it contains three databases:
- =template0=, never modified, used for restoring data or creating a db with a different encoding
- =template1=, used as a template for all other databases that can be created
- =postgres=, a regular database already

**** System Catalog
The system catalog contains the metadata of all cluster objects (tables, indexes, data types, functions...).
- All system catalog tables are prefixed by =pg_=.
- All columns are prefixed by three letters that usually correspond to the table name.
- All tables have a primary key column: =oid= (from object identifier), a 32-bit integer (type also called =oid=)

**** Schemas
Schemas are namespaces for the objects of a database.

**** Tablespaces
Tablespaces define the physical data layout of objects. A tablespace is virtually (what does virtually mean?) a directory in a file system. A tablespace can contain objects from different databases, and a database can store its objects in several tablespaces.

Interesting property:
#+begin_quote
You can distribute your data between tablespaces in such a way that archive data is stored on slow disks, while the data that is being actively updated goes to fast disks.
#+end_quote

By default, each database has its own /default tablespace/, which contains:
- All objects from the database
- All objects related to this database from the system catalog

There are also always two tablespaces:
- pg_default, the default tablespace (but doesn't each database have one for it?)
- pg_global, that contain system catalog objects common to the whole cluster

In the [[Files and Forks]] section, it's said:
#+begin_quote
Each tablespace directory (except for pg_global) contains separate subdirectories
for particular databases. All files of the objects belonging to the same tablespace
and database are located in the same subdirectory.
#+end_quote

**** Relations
All objects that consist of rows (tables, indexes, sequences, materialized views) are called relations.

**** Files and Forks
All information from a relation is stored in a collection of forks.
A fork is a collection of 1GB (historical limit that can be configured) files called segments. Each filename is of the form =<oid>_<type>.<seq>=, where:
- =<oid>= is a numeric identifier of the fork
- =<type>= is the type of the fork
- =<seq>= is the number of segment (if there is more than one file in the fork)

The types of forks are:
- The main fork: it contains the actual table rows or index rows.
- The initialization fork: available only for unlogged tables and their indexes. Actions performed on these objects are not written into the write-ahead log (WAL).
- The free space map: organized as a tree, it keeps track of available space within pages. This fork's files only appear when needed. It takes at least three pages.
- The visibility map fork: available only for tables (not for indexes), it contains two bits for each table page:
  - The first is set for pages that contain only up-to-date row versions
  - The second bit is set for pages that contain only frozen row versions.

**** Pages
The minimum amount of data that can be read or written is called a page. The default is usually =8kb= (configured up to =32kb= only during build time). The I/O is similar for all files:
1. Pages are moved to the buffer cache (where they can be read and updated by processes)
2. Pages are flushed back to disk as required.

**** TOAST
#+begin_quote
Each row must fit a single page: there is no way to continue a row on the next page.
#+end_quote
Is this a design decision? Is there any other solution that could have worked here?

When a long attribute is used in a table, it is not physically stored in the table itself, but on an ancillary table called TOAST (from The Oversized Attributes Storage Technique). This is done because every row must fit in a single page, which usually has 8192 B (ideally, each page should accommodate four rows): if one does not (e.g., a JSON or TEXT column has a lot of data that even compressed does not fit in one page), then the data is either:
- sliced into smaller "toasts"
- compressed
- both sliced and compressed
and then stored in a TOAST table.

- A TOAST table is created always for tables that contain potentially long attributes (=numeric= or =text= types, for example).
- For indexes, the TOAST mechanism can offer only compression.

There are four available strategies:
- =plain=: TOAST is not used
- =extended=: compression in a separate TOAST table
- =external=: only separate TOAST table
- =main=: first compression; if it does not help, move to separate TOAST table

See SQL queries from page 27 to see an example.

The algorithm tries to reduce the size of each row to 1/4 of a page excluding the page's header, so around 2000 bytes: this threshold can be redefined at the table level.

#+begin_quote
It may sometimes be useful to change the default strategy for some of the columns. If it is known in advance that the data in a particular column cannot be compressed (for example, the column stores JPEG images), you can set the =external= strategy for this column.
#+end_quote


The interesting thing here is that the TOAST table is read only if needed; i.e., if that specific attribute has to be read. Quoting the author:

#+begin_quote
It is one of the reasons why you should avoid using the asterisk in production solutions.
#+end_quote

*** Processes and Memory

Postgres has always used a process-based architecture because of its simplicity, but it has some disadvantages that could be solved by switching to a thread-based structure (parallel algoithms difficult to optimize, sessions bound to processes?). The whole refactor that that would imply and other challenges have stopped this change for many years.

Several processes are run to maintain server operation:
- startup
- autovacumm
- wal writer
- checkpointer
- writer
- stats collector
- wal sender
- wal receiver

This processes can be configured with dozens of parameters, but:
#+begin_quote
general considerations will only help you select more or less adequate initial values; later on, these settings have to be fine-tuned based on monitoring data.
#+end_quote

Processes interact with each other using shared memory allocated by the initial process, postmaster.

Postgres uses RAM to cache recent read pages in what's called the buffer cache, that live in the shared memory.

In case of failure, RAM is lost, so Postgres maintains the write-ahead log (WAL) to restore data when it restarts.

#+begin_quote
PostgreSQL (almost) never bypasses the operating system mechanisms to use direct I/O, so it results in double caching.
#+end_quote
I'm very interested about that /almost/.

*** Clients and the Client-Server Protocol

Everytime a new client appears, =postmaster= creates a new separate backend process, that handles the session for this client.

Having a process for every single client can have an important overhead on the server. This is solved by using a connection pooling, which makes clients share processes.

The protocol used is based on the standard =libpq= library. Each connection is done on behalf of a role or user, and to a particular database. When connecting, authentication is performed.

** Chapter 2: Isolation

*** Consistency

- Consistency = data correctness
- Data integrity at the DB level: constraints such as =NOT NULL= or =UNIQUE=.
- Data consistency is stricter than data integrity, and it's the app who must ensure it is not broken.
- Using a transaction is the way to let the DB know that a set of operations constitute a whole.
- Transactions that run concurrently can also operate incorrectly. We need to isolate them from each other.
- Full isolation is hard and affect performance, so we may accept weaker levels of isolation, leaving the job of maintaining data consistency to the application.

#+begin_quote
A transaction is a set of operations that takes the database from one correct state to another correct state (consistency), provided that it is executed in full (atomicity) and without being affected by other transactions (isolation).
#+end_quote

*** Isolation Levels and Anomalies Defined by the SQL Standard

Isolation levels, specified in the SQL standard, are defined by the anomalies that may or may not occur.

There are many more known anomalies, but the isolation levels are only defined on a few (this is probably for historical reasons, as they were the known anomalies when the standard was written). The author *believes* that the levels come from the number of locks required for their implementation, as it was assumed that isolation had to be based on locks (see Two-phase locking protocol (2pl)):
- Read uncommited: rows to update locked for writes but not for reads.
- Read commited: rows to update locked for writes and for reads.
- Repeatable read: rows to read and to update locked for writes and for reads.
- Serializable: impossible only with locks on rows, since you cannot lock a row that does not exist yet. Theoretical solution, never implemented: predicate locks (locks on conditions).

The following sections explain each of these anomalies (and in which isolation levels are allowed). But first, a summary:

| Level v / Anomaly > | Lost update | Dirty read | Non-repeatable read | Phantom read | Other anomalies |
|---------------------+-------------+------------+---------------------+--------------+-----------------|
| Read uncommited     | -           | yes        | yes                 | yes          | yes             |
| Read commited       | -           | -          | yes                 | yes          | yes             |
| Repeatable read     | -           | -          | -                   | yes          | yes             |
| Serializable        | -           | -          | -                   | -            | -               |

**** Lost Update (never allowed)

- Transactions A and B read row R.
- A and B update R based on what they read, without taking into account each other's update.

**** Dirty Reads (allowed at Read Uncommited)

- Transaction A reads and updates row R but does not commit.
- Transaction B reads updated row R.
- Transaction A then decides to roll back.

**** Non-Repeatable Reads (allowed at Read Uncommited + Read Commited)

- Transaction A reads row R.
- Transaction B updates row R to R'.
- Transaction A reads row R again, but it reads now R'.

**** Phantom Reads (allowed at Read Uncommited + Read Commited + Repeatable Read)

This is very similar to non-repeatable reads: instead of updating the row, a new row satisfying the condition is added.

- Transaction A asks for rows that satisfy condition C, and obtains S.
- Transaction B adds a new row that satisfies condition C, so the set becomes S'.
- Transaction A asks for rows that satisfy condition C, but it obtains now S'.

**** No Anomalies (only possible with Serializable)

The Serializable level prevents *any* anomalies, even the ones we don't know about, so the application does not have to take isolation into account at all.

*** Isolation Levels in PostgreSQL

PostgreSQL does not use lock-based protocols (such as 2pl), but a multiversion flavor of the Snapshot Isolation (SI) protocol:
- SI: Transactions read a snapshot of data at a particular point in time.
- Multiversion: the DB can contain several versions of one row at any point.
- It only uses locks when concurrently updating a row. Writes never lock reads, and reads never lock anything.

The implementation of PostgeSQL is a bit different from what the standard defines. Dirty reads are always forbidden, so Read uncommited and Read commited are the same.

| Level v / Anomaly > | Lost update | Dirty read | Non-repeatable read | Phantom read | Other anomalies |
|---------------------+-------------+------------+---------------------+--------------+-----------------|
| Read commited       | yes         | -          | yes                 | yes          | yes             |
| Repeatable read     | -           | -          | -                   | -            | yes             |
| Serializable        | -           | -          | -                   | -            | -               |

**** Read Committed

***** No dirty reads

If a transaction is not commited, their updates are never seen elsewhere. Snapshots, yay!

***** Non-repeatable reads

#+begin_quote
In a transaction, you must not take any decisions based on the data read by the previous operator, as everything can change in between.
#+end_quote

How to avoid these problems?
- Replace procedural code with declarative one.
- Use a single SQL operator.
- Apply explicit locks (~SELECT FOR UPDATE~ or even ~LOCK TABLE~).

***** Read skew

This is a new anomaly:
- Transaction A reads row R.
- Transaction B updates row R to R' and row Q to Q'.
- Transaction A reads row Q'.

Transaction A should have read either R and Q, or R' and Q'.

This can also happen with long-running transactions that call =VOLATILE= functions (which is the default volatility category).

Q: Do we use these types of functions in Mattermost?

***** Read skew instead of lost updates

See the whole example in the book, page 50, very interesting.

- ~UPDATE ... SET ... WHERE ... IN (SELECT ...)~ has two stages: 1. the rows to update are selected; 2. the rows are updated one by one.
- An ongoing modification of one of those rows lock it for writing, but not for reading; so step 1. is executed normally and step 2. has to wait until the row is updated, getting

Q: Do we have examples of this in Mattermost? I'm sure there are many.

***** Lost updates

Lost updates can happen if it's the application the one that stores the intermediate result.

- Transaction A reads row R and the app stores the result.
- Transaction B reads row R and the app stores the result.
- Transaction B updates row R to R' based on what the app stored.
- Transaction A updates row R to R'' based on what the app stored.

R'' should have known about R' in the first place.

**** Repeatable Read

***** No non-repeatable and phantom reads

There are no non-repeatable and phantom reads at the Repeatable read level. That is:

#+begin_quote
At this isolation level, you do not have to worry that something will change between operators.
#+end_quote

This is interesting, I should think more about it:

#+begin_quote
In the second session, letâs start another transaction, with the Repeatable Read level explicitly specified in the BEGIN command (the level of the first transaction is not important.
#+end_quote

***** Serialization failures instead of lost updates

The read skew anomaly is prevented by failing the transaction that would cause it. The error is: =ERROR: could not serialize access due to concurrent update=.

#+begin_quote
A practical insight: if your application is using the Repeatable Read isolation level for writing transactions, it must be ready to retry transactions that have been completed with a serialization failure. For read-only transactions, such an outcome is impossible.
#+end_quote

***** Write skew

This is yet another anomaly, that can happen in the Repeatable read level:
- Transaction A reads some data, and verifies that after an update U, condition C is satisfied.
- Transaction B reads some data, and verifies that after an update U, condition C is satisfied.
- If any of A or B would have known about the other one, they would have seen that after the second update U, condition C is no longer satisfied.

This one's bad.

***** Read-only transaction anomaly

A new one that Repeatable read allows. Too complex to specify in general terms. See example in the book, page 57.

**** Serializable

#+begin_quote
Thus, if an application uses the Serializable isolation level, it must be ready to retry transactions that have ended with a serialization failure. (The Repeatable Read level requires the same approach unless the application is limited to read-only transactions.)
#+end_quote

***** No anomalies

Write-skew and read-only transaction anomalies always fail with the error:

#+begin_src
ERROR: could not serialize access due to read/write dependencies among transactions
DETAIL: Reason code: Canceled on identification as a pivot, during commit attempt.
HINT: The transaction might succeed if retried.
#+end_src

***** Deferring a read-only transaction

We can defer the read-only transaction until it's safe by declaring it as =READ ONLY DEFERRABLE=.

Q: Do we need both? What are the advantages of using only =READ ONLY= or only =DEFERRABLE=? Take a look at the Postgres manual.

*** Which Isolation Level to Use?

- Read Commited: default, convenient, but all anomalies have to be kept in mind when developing. Hard to reproduce the bugs that will happen.
- Repeatable Read: modify the app to handle serialization failures. For read-only transactions, perfect complment to the Read Committed level.
- Serializable: just have to worry about retrying any transaction that fails with a serialization error. Overhead reduce system throughput. **Not supported on replicas** and cannot be combined with other isolation levels

** Chapter 3: Pages and Tuples

*** Page Structure

Each page has several parts; of which only the header has a fixed size (24 bytes):

| Part                   | Location            | Contents                                                       |
|------------------------+---------------------+----------------------------------------------------------------|
| Page header            | [0, 24)             | Page checksum, values of =lower=, =upper= and =special=        |
| Array of item pointers | [24, lower)         | Pointers to rows (4 bytes each): offset + length + status bits |
| Free space             | [lower, upper)      | Never fragmented ð                                            |
| Items (row versions)   | [upper, special)    | Row versions for tables; rows references for indexes           |
| Special space          | [special, pagesize) | Some indexes store auxiliary info                              |

The sizes of each part can be shown using the =pageinspect= extension, using:

#+begin_src sql
CREATE EXTENSION pageinspect;

SELECT lower, upper, special, pagesize
FROM page_header(get_raw_page('accounts',0));
#+end_src

The array of item pointers is the mechanism used by indexes to:
- Have unique references to rows.
- Allow moving rows around in the page to avoid fragmentation.

Thus, instead of the indexes directly referencing the row offset, they reference a pointer in that array, which in turns points to the offset of the row. This way, we can have a unique reference to the row while we can move the row around in the page (we just need to update the offset in the pointer, not the pointer itself).

*** Row Version Layout

Each row has a header followed by actual data. The header (min 23 bytes) contains, among others:
- =xmin=, =xmax=: transaction IDs used to discriminate among different versions of the same row.
- =infomask=: set of bits giving info on the versioning of the row. Two important pairs are:
  - =xmin_committed= and =xmin_aborted=: if set, the =xmin= transaction has been committed/aborted.
  - =xmax_committed= and =xmax_aborted=: if set, the =xmax= transaction has been committed/aborted.
- =ctid=: pointer to the next updated version of the same row.
- =null bitmap=: array of bits marking NULL-able columns.

The row header is often larger than the row data ð®

#+begin_quote
Data layout on disk fully coincides with data representation in RAM. The page along with its tuples is read into the buffer cache as is, without any transformations. That's why data files are incompatible between different platforms.
#+end_quote

- The size of a tuple depends on the order of fields because of data alignment.

*** Operations on Tuples

All versions of all rows are marked with two values: =xmin= and =xmax=. They are (ever-increasing) transaction IDs that define the "validity time" of the specific version:
- Create row: =xmin= is set to the transaction ID of the =INSERT= command.
- Delete row: =xmax= is set to the transaction ID of the =DELETE= command.
- Update row:
  - Old row version is "deleted": =xmax= is set to the transaction ID of the =UPDATE= command.
  - New row version is "inserted": =xmin= is set to the transaction ID of the =UPDATE= command.

**** Insert

1. Begin a transaction =T=.
2. Execute an =INSERT= command.
3. The =xmin= value of the new row version is set to the ID of =T=. =xmin_committed= and =xmin_aborted= are not modified.

â They're not modified? Or are they explicitly unset?

**** Commit (CLOG and ProcArray)

There are two additional structures in the server's shared memory:
- The commit log (CLOG): it contains two bits per transaction: one for committed, one for aborted
- The ProcArray: a list of all the active processes: for each process, its current transaction is specified.

These two structures are used whenever a **new** transaction reads the row version from the page, to know if it's not visible (active, not committed or aborted):
1. To check if it's active, it reads the ProcArray. If it's there, then it does nothing else.
2. To check if it's committed/aborted, it reads the CLOG. Once determined, this new transaction sets the =xmin_committed= and =xmin_aborted= information bits (hint bits) in the original row version, so that future transactions don't need to read the CLOG.

The sensible approach would be for the original transaction to set the =xmin_commited= and =xmin_aborted= bits itself directly, instead of setting the bits in the CLOG and letting the next transaction to set the bits in the page. But that's impossible because of two reasons:
1. At that time, it is not known whether this transaction will complete successfully.
2. If a transaction affects many pages, it may be too expensive to track them.

â I may understand 2 for performance reasons, but I don't get 1.

#+begin_quote
The flip side of this cost reduction is that any transaction (even a read-only =SELECT= command) can start setting hint bits, thus leaving a trail of dirtied pages in the buffer cache.
#+end_quote

â I don't get the bit about the trail of dirtied pages. When those bits are set, isn't it already clear what happened?

**** Delete

1. Begin a transaction =T=.
2. Execute a =DELETE= command.
3. The =xmax= value of the current row version is set to the ID of =T=. =xmin_aborted= is unset.

â Is =xmin_aborted= explicitly unset? If so, why?

**** Abort

As in =COMMIT=, nothing changes in the page itself, only the CLOG is modified. When a new transaction accesses that row, then the =xmin_aborted= bit is set.

**** Update

1. Begin a transaction =T=.
2. Execute an =UPDATE= command.
3. A virtual delete+insert happens:
   1. The =xmax= value of the old row version is set to the ID of =T=. =xmax_aborted= is unset because this transaction may be rolled back.
   2. The =xmin= value of the new row version is set to the ID of =T=.

â What happens with the other bits? It's not clear to me whether they're unmodified or explicitly unset.

*** Indexes

- Indexes do not use row versioning -> Index row headers do not contain =xmin= and =xmax= fields.
- How is visibility determined? Just access the table (there's another mechanism with the visibility map, but not explained here).

*** TOAST

- Rows of TOAST tables are handled in such a way that they are never updated; they can be either added or deleted.
- Data modifications that do not affect long values don't change the TOAST table.
- Data modifications that do affect long values force the creation of new toasts.

*** Virtual transactions

- Transactions do not get a permanent, unique, transaction ID until they perform some write. If they are read-only, they do not consume any real transaction ID.

*** Subtransactions

- When using a savepoint in a transaction, the subtransaction gets a new (higher than the parent's) transaction ID.
- Everything works the same for subtransactions, but:
  - The committed and aborted bits are set at the same time.
  - The final decision is made when the parent transaction is committed or aborted.
- The information about subtransactions is stored under the =pgdata/pg_subtrans/= directory. Same structure as CLOG buffers.
- After a failure in a transaction, all remaining changes are aborted; otherwise, atomicity would not be respected.
  - If we use ~\set ON_ERROR_ROLLBACK on~; this behaviour is not followed. To do so, Postgres implicitly encapsulates each statement in a subtransaction.

** Chapter 4: Snapshots

*** What is a Snapshot?

- A snapshot is, logically, the set of rows that are visible to a transaction. It's what puts the C in ACID (consistency).
- The moment a snapshot is taken depends on the isolation level:
  - Read Commited: at the beginning of each statement. Active only during that statement.
  - Repeateable Read and Serializable: at the beginning of the first statement of a transaction. Active during the whole transaction.

*** Row Version Visibility

- A snapshot does not contain copy of any data. It is a definition of what rows are visible when the snapshot was taken.
- Tuple visibility is defined by its =xmin= and =xmax= values. =[xmin, xmax]= intervals do not intersect, so just one version of each row can be visible.
  - Although exact rules are complex, the gist of it is: a tuple is visible in a snapshot that includes =xmin= transaction changes but excludes =xmax= transaction changes.
- The changes from a transaction are visible in a snapshot if the transaction was committed before the snapshot was taken.

*** Snapshot Structure

- Postgres does not know when a transaction is committed, only when it is started.
- But we can know what the current status of a transaction is through the =ProcArray= structure living in the server's shared memory.
- If a transaction completes, we cannot know whether it was active at some point in time; specifically, at the point a snapshot was taken. Some corollaries:
  - A snapshot needs to collect the status of all the transactions at that moment.
  - A snapshot cannot be made at some arbitrary point in the past, only now.
- Some of the values stored with a snapshot, all relative to the moment of its creation, are:
  - =xmin=: ID of the oldest active transaction. Smaller IDs are committed or aborted.
  - =xmax=: ID of the latest committed transaction plus one. Equal or larger IDs are still running or do not exist.
  - =xip_list=: IDs of all active transactions (except for virtual ones).
- See pages 90-92 for an example.
- Changes are included into a snapshot if they are made by transactions whose =xid= satisfy either of:
  1. =xid < xmin=
  2. =xmin â¤ xid < xmax= and =xid not in xip_list=

*** Visibility of Transactions' Own Changes

- A cursor is an iterator-like variable containing a reference to a query that can later be fetched.
- Cursors in a transaction need to see the state of rows when they were opened, avoiding any later changes.
- This is accomplished by:
  - Storing =cmin= and =cmax= values in tuple header: they contain the sequence number of the operation within the transaction (=cmin= for inserts, =cmax= for deletions). These values are actually stored in a single field, since it's assumed a row will not be inserted and deleted in the same transaction (the edge case is handled by a special combo identifier).
  - Storing the =cmin=/=cmax= value in the snapshot (this cannot be retrieved using SQL).
  - Comparing the tuple header =cmin=/=cmax= value with the snapshot's value. If the snapshot's value is lower than the tuple's value, then that tuple is not visible.

*** Transaction Horizon

- The horizon of a transaction is defined as its lower boundary; i.e., its =xmin= value; i.e., the ID of the oldest transaction that was active when the snapshot was created.
  - All transactions with =xid < xmin= are guaranteed to be committed.
  - A transaction can only see the row versions beyond its horizon.
- The horizon of a database is the oldest horizon of all transactions; i.e., the oldest =xmin=
  - Before this horizon, all outdated heap tuples will never be visible to any transaction.
  - Such tuples can be safely vacuumed.
- Conclusion:
  - If a transaction (real or virtual for Repeatable Read or Serializable, or real for Read Committed) is running for a long time, it holds the database horizon and defer vacuuming.
  - A virtual transaction in Read Committed holds the horizon only while executing operators.
- Transaction horizon can be seen with ~SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid()~.
#+begin_quote
In a perfect world, you should avoid combining long transactions with frequent updates (that spawn new row versions), as it will lead to table and index bloating.
#+end_quote

*** System Catalog Snapshots

- Transactions have access to the latest changes in system catalog tables, even if they happen after a snapshot is created.
- For example, an INSERT command can see new integrity constraints in a table that were added after the snapshot was created.

- â Doesn't this break isolation? The following does not clarify it:
#+begin_quote
It may seem that such behavior breaks isolation, but if the inserting transaction had accessed the accounts table, the =ALTER TABLE= command would have been blocked until this transaction completion.
#+end_quote

It turns out it was just that the wording of the sentence was a bit confusing. I had a quick chat with the author and the translator and they agreed to change the previous sentence to:
#+begin_quote
It may seem that such behavior breaks isolation, but if the inserting transaction had managed to reach the accounts table before the =ALTER TABLE= command, the latter would have been blocked until this transaction completed.
#+end_quote

The thing is that the inserting command is done after the =ALTER TABLE= command completes. If it would have been the other way around, then the =ALTER TABLE= would have blocked and added the integrity check after the inserting command had successfully added the row with the NULL value.

*** Exporting Snapshots

- Snapshots can be reused!
- Just use the =pg_export_snapshot()=, which gives an ID that can be used in another transaction with =SET TRANSACTION SNAPSHOT 'pg_export_snapshot-id'=.

** Chapter 5: Page Pruning and HOT Updates

*** Page Pruning

- Leveraging accesses to pages (to read or update), some pruning may happen when:
  - The previous =UPDATE= couldn't place a new tuple into the same page (reflected in the header).
  - The page contains more data than the =fillfactor= parameter.
- Pruning removes tuples beyond the database horizon in a single page.
- Any =SELECT= statement can cause page modifications (because it prunes old tuples).
- Pointers to pruned tuples remain in place (they may be referenced from an index).
- When pruning a page, the index page is not touched. That happens, of course, when accessing the index page itself (e.g. with an index scan).
- See example in pages 101-104.

*** HOT Updates

- When a column is updated, all indexes that reference it need to be updated as well (i.e. a new entry is added to the index) so they point to the new version of the row, since the key has changed.
- When updating a column that is not in *any* index, there is no need to add entries to indexes with the exact same key.
- An update involving columns that do not appear in any index are called Heap-Only Tuple (HOT) updates. All the other ones are called cold (or non-HOT) updates.
- In the case of a HOT update:
  - Indexes contain just one entry for all versions of a row. This entry points to the original version.
  - All subsequent versions in the same page are found following the =ctid= pointers of the tuples (the HOT chain).
  - The tuples in the HOT chain are checked for visibility in the index scans.
  - The new row versions not pointed to by any index are tagged with the HOT bit.
  - Row versions in a HOT chain that have following versions are tagged with Heap HOT Updated (HHU) bit.
- See example in p. 106.
#+begin_quote
A HOT update is possible if the modified fields are not a part of any index. Otherwise, some of the indexes would contain a reference to a heap tuple that appears in the middle of the chain, which contradicts the idea of this optimization.
#+end_quote
- â I am not sure why updating a column in an index A would not let us start a HOT chain for an index B that does not reference it, but I think I understand it now. Is this correct?
  - The case of an already started HOT chain that now updates a column in index A not referenced for index B makes sense, since we would need to have some kind of tree of chains, which is probably too costly.
  - But let's say that we *start* one HOT chain updating a column referenced by index A. In this case, this chain is only used by index B.
  - Index A still points to the new versions of the rows, which are now marked as HOT only for B.
  - We need to store that information somewhere, and the only sensible place to do so would be the tuples in the page.
  - But then, we'd need to store for which indexes that tuple is marked as HOT, which would be too costly.
  - We could also store for which indexes this tuple is *not* HOT.
  - In any case, that probably introduces too much complexity (?)
  - So we narrow the focus of HOT updates to columns that are not in *any* index.

*** Page Pruning for HOT Updates

- When page pruning happens and there's a HOT chain in the page, some special steps are taken:
  - The head of the chain needs to remain, since indexes point to it. Others can be released.
  - The head of the chain changes to a redirect state.
  - The head of the chain points now to the tuple that currently starts the chain.
  - All unused tuples are pruned, and new tuples can now reclaim their space.
- â Not sure I understand this:
#+begin_quote
If unindexed columns are modified frequently, it makes sense to reduce the fillfactor value, thus reserving some space in the page for updates. Obviously, you have to keep in mind that the lower the fillfactor value is, the more free space is left in the page, so the physical size of the table grows.
#+end_quote

*** HOT Chain Splits

- HOT chains are isolated to a single page.
- If the page runs out of space when adding a new tuple:
  - The new tuple is created in another page.
  - A new index entry is created to refer to the new tuple (which may start another HOT chain by itself).

*** Page Pruning for Indexes

- Two separate index pages are never merged into one, which leads to index bloating.
- To try to avoid this, Postgres tries to defer splitting a page as much as it can by pruning a page.
- Pruning happens when an insertion into a B-tree is about to split the page into two.
- Two types of tuples can be pruned in this case:
  - Tuples tagged as dead during an index scan because the pointed tuple is not visible or does not exist.
  - Tuples that point to different versions of the same table row.
    - Postgres check visibility of heap tuples, which require table access.
    - This is only performed for "promising" index tuples ("copies of the existing ones for MVCC purposes"â).
    - "It is cheaper to perform such a check than to allow an extra page split"

** Chapter 6: Vaccuum and Autovacuum

*** Vacuum

- Page pruning is fast but narrow-scoped.
- The =VACUUM= command processes a whole table, removing dead heap tuples *and* corresponding index entries.
- Vacuuming can happen in parallel with other processes (with some exceptions, as =CREATE INDEX= or =ALTER TABLE=).
- The visibility map tracks which pages contain only current tuples, so those can be skipped when vacuuming.
- Visibility/free space maps and vacuuming:
  - The visibility map contains the pages that only have current tuples: these can be skipped when vacuuming.
  - After vacuuming, if all remaining tuples are current, the visibility map will now include this page.
  - After vacuuming, the free space map is updated to reflect the space that has been cleared.

*** Database Horizon Revisited

- Vacuuming remove dead tuples based on the database horizon. If a transaction is holding it, some tuples may not be eliminated.
- =VACUUM VERBOSE tablename;= let us see how many tuples are eliminated or skipped and the database horizon.

*** Vacuum Phases

In short:
1. The table is scanned in search of dead tuples
2. Dead tuples are removed from the index
3. Dead tuples are removed from the table
4. Repeat 1-3 if there are too many dead tuples
5. Perform heap truncation

**** 1. Heap Scan

- All pages in the visibility map are skipped
- For all other pages: if a tuple is beyond the horizon, its ID is added to a =tid= array.
- The =tid= array is in the local memory of the =VACUUM= process
- The whole memory chunk (size defined by =maintenance_work_mem= parameter) is allocated at once.
- â Doesn't the following contradict the previous affirmation?
#+begin_quote
the allocated memory never exceeds the volume required in the worst-case scenario, so if the table is small, vacuuming may use less memory than specified in this parameter.
#+end_quote
- Two possible outcomes:
  - The table is scanned in full
  - The =tid= array is filled before the scanning finishes
- In either case, we continue with the next phase.

**** 2. Index Vacuuming

- For *each* index in the table (even if it's not the first time we scanned the heap)
  - The index is *fully* scanned to find entries that refer to tuples in the =tid= array.
  - Such entries are removed from the index pages.
  - The free space map is updated.
  - Statistics on vacuuming are collected.
- This process can be run in parallel with a single worker per index.
- If rows are only inserted (so the =tid= array is empty because there are no dead tuples) this phase is skipped, but an index scan is forced once at the end as part of a separate phase of *index cleanup*.

**** 3. Heap Vacuuming

- The table is scanned again to:
  - Remove tuples registered in the =tid= array.
  - Free the corresponding pointers.
- The free space and the visibility maps are updated.
- If the table was not fully scanned in 1., the =tid= array is cleared and 1. is resumed.

**** 4. Heap Truncation

- In the previous phase, we can end up with several empty pages at the end of the file (a tail). â How are all in the end?
- If that chunk of free space is at least 1/16 of the table or there are at least 1000 pages, then:
  - The =VACUUM= process locks the table (for a short time).
  - This tail is returned to the operating system as free space.

*** Analysis

- The =ANALYZE= command collects statistical information for the query planner, such as:
  - Number of rows (=pg_class.reltuples=).
  - Number of pages (=pg_class.relpages=).
  - Data distribution within columns.
- The =VACUUM ANALYZE= command appeared before a separate =ANALYZE= command.
- Even if running =VACUUM ANALYZE=, these two processes are performed sequentially.
- However, automatic vacuum and analysis are set up in a similar way.

*** Automatic Vacuum and Analysis

- Autovacuum launches vacuum and analysis processes based on the intensity of table updates.

**** About the Autovacuum Mechanism

- The =autovacuum launcher= process is always running (if =autovacuum= and =track_counts= parameters are enabled).
- It defines the autovacuum schedule and maintains a list of active databases.
- Once in =autovacuum_naptime= (1min by default):
  - An =autovacuum worker= starts for each active database (always less workers than =autovacuum_max_workers=)
  - The worker connects to the corresponding database and builds:
    - a list of all tables, materialized views and TOAST tables to be vacuumed
    - a list of all tables and materialized views to be analyzed (TOAST tables don't need to be analyzed because they're always accessed via an index)
  - The objects in the list are vacuumed or/and analyzed
  - The worker is terminated.
  - If the worker doesn't finish in =autovacuum_naptime=, another worker is spawned to process different tables in parallel (there's no parallelism at the table level).
- Differences between regular vacuum and autovacuum:
  - Separate parameters to define =tid= arrays: =maintenance_work_mem= and =autovacuum_work_mem=. By default, =autovacuum_work_mem= equals =maintenance_work_mem=.
  - Parallel processing a table's indexes happen only for vacuum; not allowed for autovacuum.

**** Which Tables Need to be Vacuumed?

- The statistics collector is constantly updating two counters in the =pg_stat_all_tables= table:
  - =n_dead_tup=: Number of dead tuples in a table.
  - =n_ins_since_vacuum=: Number of rows inserted since last vacuum.
- Vacuuming is triggered for two reasons:
  1. Dead tuple accumulation
     - Vacuum happens if =n_dead_tup > abs + rel * n_tup=, where:
       - =abs= is controlled by =autovacuum_vacuum_threshold=, an absolute number of dead tuples.
       - =rel= is controlled by =autovacuum_vacuum_scale_factor=, a fraction of dead tuples in a table.
       - =n_tup= is the =pg_class.reltuples= counter, an estimate of the number of live rows in a table.
  2. Insertion of new rows
     - If rows are only inserted, there are no dead tuples.
     - However, in order to enable index-only scans, vacuuming needs to happen to freeze heap tuples in advance and update the visibility map.
     - Vacuum happens if =n_ins_since_vacuum > abs + rel * n_tup=, where:
       - =abs= is controlled by =autovacuum_vacuum_insert_threshold=, an absolute number of rows inserted since last vacuum.
       - =rel= is controlled by =autovacuum_vacuum_insert_scale_factor=, a fraction of new rows in a table since last vacuum.
       - =n_tup= is the =pg_class.reltuples= counter, an estimate of the number of live rows in a table.
- All the previous thresholds can be overridden at the table level.
  - The default value of =autovacuum_vacuum_scale_factor= is 20%, which may be too big for large tables.
  - The optimal parameters depend on the table size and workload type (but no tips are given).
  - There are corresponding parameters for the thresholds for TOAST tables, with the same name and prefixed by =toast.=.

**** Which Tables Need to Be Analyzed?

- Analysis processes only modified rows, so the frequency depends only on that.
- The statistics collector is constantly updating one counter more in the =pg_stat_all_tables= table:
  - =n_mod_since_analyze=: Number of rows modified in a table since last analysis.
- Analysis happens if =n_mod_since_analyze > abs + rel * n_tup=, where:
  - =abs= is controlled by =autovacuum_vacuum_analyze_threshold=, an absolute number of rows inserted since last vacuum.
  - =rel= is controlled by =autovacuum_vacuum_analyze_scale_factor=, a fraction of new rows in a table since last vacuum.
  - =n_tup= is the =pg_class.reltuples= counter, an estimate of the number of live rows in a table.
- All the previous thresholds can be overridden at the table level.
- TOAST tables are not analyzed.

**** Autovacuum in Action

- See example in pages 126-130
- Typo in page 128: it should read 1000 instead of 1006

*** Managing the Load

- Vacuuming can have a noticeable impact on performance even if it does not block other processes.

**** Vacuum Throttling

- Manual vacuum can be throttled with two parameters:
  - =vacuum_cost_limit=: controls the amount of work done after which the process sleeps (default 200)
    - Each page read from the buffer cache costs =vacuum_cost_page_hit= (default 1)
    - Each page read not in the buffer cache costs =vacuum_cost_page_miss= (default 2)
    - Each page dirtied by vacuum (i.e., that needs to ) costs =vacuum_cost_page_dirty= (default 20)
  - =vaccum_cost_delay=: controls the amount of time the process is sleeping (default 0)
- With default values, between 9 and 200 pages per cycle are processed.

**** Autovacuum Throttling

- Same two parameters, prefixed by =autovacuum=:
  - =autovacuum_vacuum_cost_limit=: defaults to -1, which means fallback to =vacuum_cost_limit=.
    - No different =cost_page_{hit,miss,dirty}=.
  - =autovacuum_vaccum_cost_delay=: defaults 2ms, -1 means fallback to =vacuum_cost_delay=.
- These values are shared between all workers
  - Regardless of the number of workers, the impact on the system is similar
  - If autovacuum needs to be sped up, both =autovacuum_max_workers= and =autovaccum_vacuum_cost_limit= should be increased.
- These values can be overridden by table (same name) and for TOAST tables (same name prefixed by =toast.=).

*** Monitoring

- Monitoring vacuum processes can detect cases where dead tuples cannot be removed in one go (because they don't fit in the =maintenance_work_mem=-sized memory chunk).
  - In this case, all indexes have to be fully scanned several times
  - This can be fixed by either vacuuming more often or allocating more memory

**** Monitoring Vacuum

- Run =VACUUM= with the =VERBOSE= clause
- Views with current state of started processes:
  - =pg_stat_progress_vacuum= for vacuum.
  - =pg_stat_progress_analyze= for analyze (usually very fast and unlikely to cause issues).
-

**** Monitoring Autovacuum

- Set =log_autovacuum_min_duration= to something other than the default value of -1
- To track tables that need vacuuming or analysis, we can use the views defined in the chapter (=need_vacuum= and =need_analyze=).
  - If the list of tables grows, either reduce =autovacuum_vacuum_cost_delay= or increase =autovacuum_vacuum_cost_limit= (maybe also increasing =autovacuum_max_workers=).

** Chapter 7: Freezing

*** Transaction ID Wraparound

- Transaction ID: 32 bits (~4 billion possible IDs)
  - 64-bits transaction IDs would be too large for the two (=xmin= and =xmax=) IDs in each tuple's header.
- When exhausted, the counter has to be reset. This is called a wraparound.
  - Age of a transaction T: the number of subsequent transactions that have appeared since the start of T.
  - To handle wraparound, Postgres compare the age of transactions instead of transaction IDs.
  - We use the terms older and younger instead of less and greater.
#+begin_src c
// TransactionIdPrecedes --- is id1 logically < id2?
bool TransactionIdPrecedes(TransactionId id1, TransactionId id2) {
    // If either ID is a permanent XID then we can just do unsigned
    // comparison.  If both are normal, do a modulo-2^32 comparison.
    int32 diff;

    if (!TransactionIdIsNormal(id1) || !TransactionIdIsNormal(id2))
        return (id1 < id2);

    diff = (int32) (id1 - id2);
    return (diff < 0);

    id1 - 1d2 < 0
}
#+end_src
  - â Sooner or later, an old transaction that was once in the remote past will be seen as a future one. This has to be handled; otherwise, no snapshot would see such transaction anymore.

*** Tuple Freezing and Visibility Rules

- To prevent the above, vacuuming also searches for tuples beyond the database horizon and tags them in a special way; i.e., it freezes them.
- Frozen tuples are visible in all snapshots, so their =xmin= is ignored, and that ID can be safely reused.
- =xmin= is unchanged in the tuple; freezing is marked by the combination of both =committed= and =aborted= hint bits.
- =xmax= does not participate in freezing in any way (it's only present in outdated tuples). When =xmax= is beyond the database horizon, such tuples will be vacuumed away.

*** Managing Freezing

- Four parameters control freezing:
| Parameter                   | Default | Description                            |
|-----------------------------+---------+----------------------------------------|
| =vacuum_freeze_min_age=     | 50 M    | When does freezing start?              |
| =vacuum_freeze_table_age=   | 150 M   | When is aggressive freezing performed? |
| =autovacuum_freeze_max_age= | 200 M   | When is freezing forced?               |
| =vacuum_failsafe_age=       | 1600 M  | When does freezing receive priority?   |

**** Minimal Freezing Age

- =vacuum_freeze_min_age= defines the minimal age of =xmin= transactions that are going to be frozen.
- Lower value => higher overhead. If a row is actively being changed, freezing all new versions is a wasted effort.
- This parameter can be overridden at the table level (normal and TOAST tables).

**** Age for Aggressive Freezing

- =vacuum_freeze_table_age= defines the transaction age that allows vacuuming to ignore the visibility map, so any heap page can be frozen.
- Each table has an associated transaction ID for which all older transactions are sure to be frozen. It's in =pg_class.relfrozenxid=.
- This transaction's age is compared to the =vacuum_freeze_table_age=  value to decide whether to start aggressive freezing.
- The freeze map allows vacuuming to:
  - skip pages that are in the map
  - ensure fault tolerance: if vacuuming is interrupted, the next run will not need to process the pages already in the map, so no work is repeated.
- Aggressive freezing of all pages is performed when the number of transactions in the system reaches =vacuum_freeze_table_age - vacuum_freeze_min_age= (100 M with default values)
  - If =vacuum_freeze_min_age= is too big => higher overhead. â
- This parameter can be overridden at the table level (normal and TOAST tables).

**** Age for Forced Autovacuum

- Autovaccum is forced, even if switched off, when there's risk that some unfrozen transaction IDs exceed the =autovacuum_freeze_max_age= value.
- Decision taken based on the age of the oldest =pg_class.relfrozenxid= in all tables.
- That ID is stored in =pg_database.datfrozenxid=.
  - â Is =datfrozenxid= the database horizon? We think this is always older than the horizon.
- The value of =autovacuum_freeze_max_age= must be in [100k, 2000M].
  - 2000M is a bit less than the max transaction ID.
  - A big value increases the risk of wraparound, as postgres may fail to timely freeze all required tuples. In such a case, the server must stop immediately!
- The =autovacuum_freeze_max_age= affects the size of CLOG, since there is no need to keep the status of frozen transactions and all transactions preceding =datfrozenxid= are sure to be frozen.
- Changing =autovacuum_freeze_max_age= requires a server restart.
- This parameter can be overridden at the table level (normal and TOAST tables).

**** Age for Failsafe Freezing

- If autovacuum cannot avoid the risk of wraparound, =autovacuum_vacuum_cost_delay= is ignored and it stop vacuuming indexes to freeze heap tuples instead.
- This failsafe freezing mode is enabled if there is a risk that the age of an unfrozen transaction in the database will exceed the =vacuum_failsafe_age= value, which is assumed to be higher than =autovacuum_freeze_max_age=.

*** Manual Freezing

**** Freezing by Vacuum

- =VACUUM FREEZE= will freeze all heap tuples as if =vacuum_freeze_min_age= is 0.
- If this is done, it makes sense to disable index vacuuming. Two alternatives:
  - =VACUUM (freeze, index_cleanup false)=
  - Via the =vacuum_index_cleanup= storage parameter: =ALTER TABLE t SET (vacuum_index_cleanup = FALSE)=.

**** Freezing Data at the Initial Loading

- =COPY ... WITH FREEZE= inserts and freezes data at once.
  - Useful for data that is not expected to change.
  - Tuples can be frozen during the initial loading only if the resulting table has been created or truncated within the same transaction, as both these operations acquire an exclusive lock on the table.
  - Isolation level can be broken here! T1 begins; T2 truncates a table, copy-freeze data and commit; and then T1 can see that data!
    - This is allowed because data loading is unlikely to happen regularly. "In most cases it will not cause any issues".
  - When loading data with freezing, visibility map is created at once, and page headers receive the visibility attribute (so it won't be processed by vacuum while data remains unchanged).

** Chapter 8: Rebuilding Tables and Indexes

*** Full Vacuuming

**** Why is Routine Vacuuming not Enough?

- =VACUUM= can rarely reduce the number of pages: only if several empty pages appear at the very end (this does not happen often).
- Consequences of excessive size:
  - Table/index scans will take longer.
  - Bigger buffer cache may be required (data density decreases).
  - B-trees can get an extra level => slower index access.
  - Files take up extra space on disk and in backups.
- If data density is too low, =VACUUM FULL= rebuilds the table and index from scratch, packing the data as densely as possible.
- In =VACUUM FULL=:
  - Postgres rebuilds the table and then each of its indexes.
  - Both old and new files for each object have to be stored on disk.
  - The access to the table is fully blocked for reads and writes.

**** Estimating Data Density

- Exact density, too expensive for large objects:
  - For tables:  =pgstattuple('tablename')=, column =tuple_percent=.
  - For indexes: =pgstatindex('indexname')=, column =avg_leaf_density=.
- Less accurate: estimate =data volume / file size=. See [[https://wiki.postgresql.org/wiki/Show_database_bloat][these huge queries]].
- For simply getting the sizes of the table and all its indexes:
  - For tables:  =pg_size_pretty(pg_table_size('tablename'))=.
  - For indexes: =pg_size_pretty(pg_indexes_size('tablename'))=.
- For tracking progress of =VACUUM FULL= processes: =SELECT * FROM pg_stat_progress_cluster=.
- After a =VACUUM FULL=, it may happen that the data density of indexes is bigger than after only inserting data.
#+begin_quote
it is more efficient to create a B-tree from scratch based on the available data than to insert entries row by row into an already existing index
#+end_quote


**** Freezing

- When rebuilding the table, Postgres freezes its tuples, since its cost is negligible compared to all the other work.
- But pages are not registered in the visibility map nor in the freeze map, and headers do not receive the visibility attribute
- This only happens after vacuuming (manual or auto)
#+begin_quote
It essentially means that even if all tuples in a page are beyond the database horizon, such a page will still have to be rewritten.
#+end_quote


*** Other Rebuilding Methods

**** Alternatives to Full Vacuuming

- =CLUSTER=
  - The same as =VACUUM FULL=, but it also reorders tuples in files based on one of the available indexes (further updates will break this physical order)
  - =VACUUM FULL= is literally a special instance of =CLUSTER= without reordering.
- =REINDEX=
  - Rebuilds one or more indexes.
  - =CLUSTER= and =VACUUM FULL= use this command under the hood.
- =TRUNCATE=
  - Deletes all table rows, so logically equivalent to =DELETE= without a =WHERE= clause. But:
    - =DELETE= marks tuples as deleted (they have to be vacuumed afterwards).
    - =TRUNCATE= creates a new empty file (usually faster).

**** Reducing Downtime during Rebuilding

- =VACUUM FULL= takes an exclusive lock for the whole operation: not suitable for high available systems. There are some alternatives:
  - =pg_repack= creates a new repacked table and replace the old with the new in the system catalog.
  - =pgcompacttable= performs multiple fake row updates without changing data, so current row versions move towards the start of the file.
    - Between updates, regular vaccuuming removes outdated tuples and truncates little by little.
    - This takes much more time and resources, but it requires no extra space.

*** Preventive Measures

**** Read-Only Queries

- File bloating can happen when long-running transactions hold the database horizon alongside intensive data updates.
- A solution is to split queries into two servers:
  - Fast OLTP queries are kept in the main server.
  - Slow OLAP transactions are redirected to a replica.
- If those long transactions are caused by something else (app bugs, driver bugs...), the server can be tuned with:
  - =old_snapshot_threshold=: maximum lifetime of a snapshot after which the server removes outdated tuples. If a transaction still requires them, it errors out (snapshot too old).
  - =idle_in_transaction_session_timeout=: lifetime of an idle transaction, after which it's aborted.

**** Data Updates

- Another reason of bloating: simultaneous modification of a large number of tuples.
- A simple solution: reduce number of changes performed by a single transaction. Something like:
#+begin_src sql
WITH batch AS (
    SELECT ID
    FROM tablename
    WHERE filtering the already processed rows
    LIMIT batch size
    FOR UPDATE SKIP LOCKED
)
UPDATE tablename SET update clause
WHERE id IN (SELECT id FROM batch);
#+end_src
- The =batch= definition selects and immediately locks a set of rows that does not exceed the specified size. The rows that are already locked by other transactions are skipped: they will get into another batch next time.
- See example in pages 158-160.
